Second Project Report 

Project Title
Real-time Anomaly Detection in Financial Transactions
Authors and Team
● Author 1: Haozhen Guo
● Author 2: Yang Liu
Team name: Black Thym
1. Executive Summary
1.1. Decisions to be impacted
Based on our research, anomaly detection algorithms play a crucial role in shaping business decisions across three key areas:
● Personal Savings Protection: By identifying abnormal transactions, banks and other financial institutions can proactively monitor and block suspicious or high-risk activities, thereby safeguarding individual savings and account security.
● Risk Management: Anomaly detection enables financial institutions to recognize unusual patterns within portfolio management, allowing for timely adjustments to investment strategies, and ultimately enhancing risk management and financial performance.
● Anti-Money Laundering (AML): A major application of anomaly detection is in identifying irregular transaction behaviors that may indicate potential money laundering activities, helping institutions comply with regulations and prevent financial crime.

1.2. Business Value
● Enhanced Security and Customer Trust
● Cost Reduction through Automated Monitoring
● Scalable Risk Management Solutions
● Proactive Fraud Prevention for Industry Growth

1.3. Data Assets
● Transaction Data
● Historical Fraud Data
● Customer Behavioral Data
● External Economic Data
2. Data Preprocessing
2.1. Data Description
2.2. Correlation with the target column

Before dropping columns with a high percentage of NaN or dominated values, we first calculate the correlation between each column and the target column (“isFraud”). This helps us identify features that may strongly correlate with the target when a transaction is fraudulent. After performing the calculations, the highest absolute correlation value is 0.396, indicating that no feature has a strong correlation with the target column.

2.3. Drop Nan and dominated features

We use two methods to analyze the distribution of NaN values in our dataset. First, we create a table that records each feature's data type, percentage of null values, and the number of unique values, which helps us evaluate the characteristics of each feature. Next, we calculate the proportion of NaN values to determine an appropriate threshold for handling missing data. The following table shows the proportion of NaN values:

From the table, it is evident that when the NaN percentage is in the range [0, 0.15], we retain half of the features, which is sufficient for model training. Additionally, there are no features with NaN percentages between 0.15 and 0.40. A 15% threshold for NaN values is significantly more manageable compared to 40%, making it a preferable choice for data retention. 


2.4. Filling Nan
2.4.1. Correlation heatmap of Features with entire values
2.4.2. Simply filling
2.4.3. KNN filling
2.5. Outlier detections
2.5.1. IQR detection
2.5.2. Z-score detection
2.5.3. DBScan/ Isolation
3. Model Updates
3.1. Models in Planning
3.2. Machine Learning WorkFLow as MLM
4. Source Code
// when finished the code, paste the github link.
5. Next Steps
5.1. Plan for Improvement
5.2. Timeline of Next Steps
